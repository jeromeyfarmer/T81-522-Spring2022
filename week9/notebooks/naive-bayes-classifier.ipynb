{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes Classifier Algorithms\n",
    "\n",
    "\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem . Bayes theorem from a classification standpoint can be specified in the following form fiven a class variable and dependent feature vectors:\n",
    "\n",
    "![](../files/bayes.png)\n",
    "\n",
    ",Where the term in the left side is the probability of a certain class variable (yj) given predictor variables X .\n",
    "\n",
    "\n",
    "After assuming the conditional independence of all xi variables for a given class, the class probability for class yj can be computed as joint probability of all xi given yj as follows:\n",
    "\n",
    "![](../files/posterior.png)\n",
    " \n",
    "Based upon the above, an observation will be assigned a class label that has the greatest conditional probability. Naive Bayes classifiers have found success in problems related to text classification and have been  used in email spam filtering and sentiment analysis problems. Naive Bayes models are fast and simple classification algorithms even for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classification with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.30, random_state = 1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB\n",
    "lr = gnb()\n",
    "lr.fit(X_train_std, y_train);\n",
    "\n",
    "from sklearn import metrics\n",
    "y_pred = lr.predict(X_test_std);\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "kmeans.labels_\n",
    "array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
    ">>> kmeans.predict([[0, 0], [12, 3]])\n",
    "array([1, 0], dtype=int32)\n",
    ">>> kmeans.cluster_centers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
